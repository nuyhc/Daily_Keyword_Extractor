{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 멋쟁이 사자처럼 AI School 7기 데일리 키워드 추출 및 워드클라우드 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Setting\n",
    "!pip install sentence_transformers konlpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# For Word Cloud\n",
    "from wordcloud import WordCloud\n",
    "# For konlpy & TA/NLP\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# For Goolge Sheet\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from google.auth import default\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)\n",
    "# Load Model\n",
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet Load and Preprocessing\n",
    "sheet_name = input(\"응답 시트 파일명 (해당 공유 폴더가 내 드라이브에 있어야함!): \")\n",
    "worksheet = gc.open(sheet_name).sheet1\n",
    "rows = worksheet.get_all_values()\n",
    "df = pd.DataFrame.from_records(rows)\n",
    "df.fillna(\"-\")\n",
    "df = df.iloc[:, -1][1:]\n",
    "temp = df.str.split(\"\\n\")\n",
    "temp = list(itertools.chain(*temp))\n",
    "intergrated = \"\\n\".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyBERT\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_doc = okt.pos(intergrated)\n",
    "tokenized_nouns = \" \".join([word[0] for word in tokenized_doc if (word[1]==\"Noun\" or word[1]==\"Alpha\")])\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "doc_embedding = model.encode([intergrated])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "top_n = int(input(\"몇개의 키워드를 추출할 것인지?: \"))\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "wc = WordCloud(\n",
    "    font_path = \"/content/drive/MyDrive/malgun.ttf\",\n",
    "    width=500,\n",
    "    height=500,\n",
    "    max_font_size=250,\n",
    "    scale=1.2,\n",
    "    background_color=\"black\"\n",
    ").generate_from_frequencies(Counter(str(tokenized_nouns).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw, end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7051211f7d0cb84f9a5276aced427e23f2f109de898d0e63a1c129a12d67d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
